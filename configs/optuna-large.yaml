# ------------------------  PyTorch Lightning Configurations --------------------------------------
seed: 12                                  # Training seed set everywhere
verbose: True                            # Verbosity level

# ----------------------------- Early Stopping ----------------------------------------------------
monitor: macro_f1                         # Metric to monitor during training
min_delta: 0.0                            # Sensitivity to the metric.
patience: 2                               # Number of epochs without improvement before stopping training    
metric_mode: max                          # 'min' or 'max' depending if we wish to maximize or minimize the metric

# ----------------------------- Model Checkpoint --------------------------------------------------
save_top_k: 1                             # How many checkpoints we want to save.
save_weights_only: True                   # Saves the model weights only

# ----------------------------- Lightning Trainer --------------------------------------------------
gradient_clip_val: 1.0                    # Clips gradients when the norm value exceeds 1.0
gpus: 1                                   # Number of GPUs to use. (1 is recommended)
deterministic: True                       # if true enables cudnn.deterministic. Might make your system slower, but ensures reproducibility.
overfit_batches: 0.0                      # DEGUB: Uses this much data of the training set. If nonzero, will use the same training set for validation and testing.
batch_size: 1
accumulate_grad_batches: 8               # Gradient accumulation steps
min_epochs: 1                             # Min number of epochs
max_epochs: 3                             # Max number of epochs
precision: 16
val_check_interval: 0.5

pretrained_model: xlm-roberta-large
dropout: 0.1249
nr_frozen_epochs: 0.1
keep_embeddings_frozen: True
learning_rate: 0.00023628315040013953
layerwise_decay: 0.925050299197838
encoder_learning_rate: 2.5745367673749778e-05
cap_loss: 1
punct_loss: 2
language_factors: False