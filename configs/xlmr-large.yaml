# ------------------------  PyTorch Lightning Configurations --------------------------------------
seed: 12                                  # Training seed set everywhere
verbose: False                            # Verbosity level

# ----------------------------- Early Stopping ----------------------------------------------------
monitor: macro_f1                        # Metric to monitor during training
min_delta: 0.0                            # Sensitivity to the metric.
patience: 1                               # Number of epochs without improvement before stopping training    
metric_mode: max                          # 'min' or 'max' depending if we wish to maximize or minimize the metric

# ----------------------------- Model Checkpoint --------------------------------------------------
save_top_k: 1                             # How many checkpoints we want to save.
save_weights_only: True                   # Saves the model weights only

# ----------------------------- Lightning Trainer --------------------------------------------------
gradient_clip_val: 1.0                    # Clips gradients when the norm value exceeds 1.0
gpus: 1                                   # Number of GPUs to use. (1 is recommended)
deterministic: True                       # if true enables cudnn.deterministic. Might make your system slower, but ensures reproducibility.
overfit_batches: 0.0                      # DEGUB: Uses this much data of the training set. If nonzero, will use the same training set for validation and testing.
batch_size: 2
accumulate_grad_batches: 8                # Gradient accumulation steps
min_epochs: 1                             # Min number of epochs
max_epochs: 3                             # Max number of epochs
precision: 16

#limit_train_batches: 100
#limit_val_batches: 20 

pretrained_model: xlm-roberta-large
dropout: 0.1
nr_frozen_epochs: 0.15
keep_embeddings_frozen: True
learning_rate: 6.0e-5
layerwise_decay: 0.95
encoder_learning_rate: 3.0e-5
binary_loss: 1
punct_loss: 1